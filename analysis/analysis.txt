Siyun Lee
ssl36

(1) Run the program BenchmarkForAutocomplete and copy/paste the 
results here this for #matches = 20

search  size    #match  binary  brute
        456976  20      0.2181  0.0149
a       17576   20      0.0045  0.0117
b       17576   20      0.0057  0.0072
c       17576   20      0.0068  0.0092
x       17576   20      0.0079  0.0087
y       17576   20      0.0083  0.0088
z       17576   20      0.0082  0.0090
aa      676     20      0.0002  0.0091
az      676     20      0.0002  0.0104
za      676     20      0.0003  0.0141
zz      676     20      0.0004  0.0112

(2) Run the program again for #matches = 10000, paste the results, 
and then make any conclusions about how the # matches 
effects the runtime. 

search  size    #match  binary  brute
        456976  10000   0.2449  0.0296
a       17576   10000   0.0047  0.0134
b       17576   10000   0.0045  0.0097
c       17576   10000   0.0056  0.0092
x       17576   10000   0.0049  0.0101
y       17576   10000   0.0070  0.0146
z       17576   10000   0.0079  0.0098
aa      676     10000   0.0001  0.0067
az      676     10000   0.0002  0.0080
za      676     10000   0.0002  0.0074
zz      676     10000   0.0003  0.0079

The number of matches did not seem to have a significant impact on the runtime for binary.
However, it does seem to slightly decrease the runtime for brute.

(3) Copy/paste the code from BruteAutocomplete.topMatches below. 
Explain what the Big-Oh complexity of the entire loop: 
for(Term t : myTerms) {...} 
is in terms of N, the number of elements in myTerms and 
M, the number of terms that match the prefix. 
Assume that every priority-queue operation runs in O(log k) time. 
Explain your answer which should be in terms of N, M, and k.


	public List<Term> topMatches(String prefix, int k) {
		if (k < 0) {
			throw new IllegalArgumentException("Illegal value of k:"+k);
		}
		
		// maintain pq of size k
		PriorityQueue<Term> pq = new PriorityQueue<Term>(10, new Term.WeightOrder());
		for (Term t : myTerms) {
			if (!t.getWord().startsWith(prefix))
				continue;
			if (pq.size() < k) {
				pq.add(t);
			} 
			else if (pq.peek().getWeight() < t.getWeight()) {
				pq.remove();
				pq.add(t);
			}
		}
		int numResults = Math.min(k, pq.size());
		LinkedList<Term> ret = new LinkedList<>();
		for (int i = 0; i < numResults; i++) {
			ret.addFirst(pq.remove());
		}
		return ret;
	}

The for loop iterates through a total of N terms, checks to see if the terms start with the
prefix, uses the M terms found to do various priority-queue operations. As a result, this
for loop will run at O(N + M*log k). This complexity shows that the for loop will run N
times to at least check if the word starts with the prefix, as shown by O(N). There will be
M such elements that match it, so it will do the various priority-queue operations M times, 
which leads it to be O(M * log k). As a result, this for loop has a compelxity of 
O(N + M*logk k). 


(4) Explain why the last for loop in BruteAutocomplete.topMatches 
uses a LinkedList (and not an ArrayList) 
AND why the PriorityQueue uses Term.WeightOrder to get 
the top k heaviest matches -- rather than 
using Term.ReverseWeightOrder.

BruteAutocomplete.topMatches uses a LinkedList because taking out the first term does not 
require the list to shift all elements down. This allows the runtime to be a lot faster.

PriorityQueue uses WeightOrder rather than ReverseWeightOrder because it removes the first
term in the list once it becomes too big. This means the first term should be the lowest 
weight, which is how WeightOrder sorts things. 


(5) Explain what the runtime of the 
BinarySearchAutocomplete.topMatches code that you 
implemented is by copy/pasting the code below 
and explaining your answer in terms of N, M, and k.

The runtime of BinarySearchAutocomplete that I implemented should be O(log N + M log M + k).
This is because there are three components to this code: calling FirstIndex and LastIndex,
sorting the M elements found between First and Last, and returning the top k weightiest
results. Both FirstIndex and LastIndex are binary searches, so it takes log N time. As a
result, if you add all these components up, you get O(log N + M log M + k).

